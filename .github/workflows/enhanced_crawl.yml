name: Enhanced Naver Cafe Crawling

on:
  schedule:
    # Run every 3 hours with jitter to avoid pattern detection
    - cron: '17 */3 * * *'  # 17 minutes past every 3rd hour
  workflow_dispatch:
    inputs:
      debug_mode:
        description: 'Enable debug mode'
        required: false
        default: 'false'
      use_mobile:
        description: 'Use mobile crawling'
        required: false
        default: 'false'

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      max-parallel: 1  # Prevent concurrent runs
      fail-fast: false
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
    
    - name: Setup Chrome
      uses: browser-actions/setup-chrome@latest
      with:
        chrome-version: stable
    
    - name: Setup ChromeDriver
      uses: nanasess/setup-chromedriver@v2
    
    - name: Cache pip packages
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Cache browser sessions
      uses: actions/cache@v4
      with:
        path: sessions
        key: ${{ runner.os }}-sessions-${{ hashFiles('session_manager.py') }}
        restore-keys: |
          ${{ runner.os }}-sessions-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install undetected-chromedriver  # Additional anti-detection
    
    - name: Verify Chrome installation
      run: |
        chrome --version
        chromedriver --version
    
    - name: Add random delay (anti-pattern detection)
      run: |
        # Random delay between 0-300 seconds to avoid pattern detection
        DELAY=$((RANDOM % 300))
        echo "Waiting ${DELAY} seconds before starting..."
        sleep ${DELAY}
    
    - name: Run enhanced crawler
      env:
        NAVER_ID: ${{ secrets.NAVER_ID }}
        NAVER_PW: ${{ secrets.NAVER_PW }}
        NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
        NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
        NOTION_TITLE_FIELD: ${{ secrets.NOTION_TITLE_FIELD }}
        CAFE1_NAME: ${{ secrets.CAFE1_NAME }}
        CAFE1_URL: ${{ secrets.CAFE1_URL }}
        CAFE1_CLUB_ID: ${{ secrets.CAFE1_CLUB_ID }}
        CAFE1_BOARD_ID: ${{ secrets.CAFE1_BOARD_ID }}
        CAFE1_BOARD_NAME: ${{ secrets.CAFE1_BOARD_NAME }}
        CAFE2_NAME: ${{ secrets.CAFE2_NAME }}
        CAFE2_URL: ${{ secrets.CAFE2_URL }}
        CAFE2_CLUB_ID: ${{ secrets.CAFE2_CLUB_ID }}
        CAFE2_BOARD_ID: ${{ secrets.CAFE2_BOARD_ID }}
        CAFE2_BOARD_NAME: ${{ secrets.CAFE2_BOARD_NAME }}
        GITHUB_ACTIONS: true
        DEBUG_MODE: ${{ github.event.inputs.debug_mode }}
        USE_MOBILE: ${{ github.event.inputs.use_mobile }}
        # Enhanced settings
        MAX_RETRIES: 5
        REQUEST_DELAY_MIN: 3
        REQUEST_DELAY_MAX: 8
        ENABLE_SESSION_ROTATION: true
        ENABLE_USER_AGENT_ROTATION: true
      run: |
        python enhanced_main.py || python enhanced_main.py  # Retry once on failure
    
    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: crawler-logs-${{ github.run_number }}
        path: |
          crawler.log
          sessions/
          debug_screenshots/
        retention-days: 7
    
    - name: Upload debug screenshots
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: debug-screenshots-${{ github.run_number }}
        path: debug_screenshots/
        retention-days: 3
    
    - name: Check crawl results
      run: |
        if [ -f crawler.log ]; then
          echo "=== Last 50 lines of crawler log ==="
          tail -n 50 crawler.log
          
          # Check for successful extractions
          SUCCESS_COUNT=$(grep -c "✅" crawler.log || true)
          ERROR_COUNT=$(grep -c "❌" crawler.log || true)
          
          echo "=== Crawl Statistics ==="
          echo "Successful operations: ${SUCCESS_COUNT}"
          echo "Failed operations: ${ERROR_COUNT}"
          
          # Fail if too many errors
          if [ "$ERROR_COUNT" -gt "$SUCCESS_COUNT" ]; then
            echo "⚠️ Warning: More errors than successes"
            exit 1
          fi
        fi
    
    - name: Clean up old sessions
      if: always()
      run: |
        # Remove sessions older than 7 days
        find sessions -type f -mtime +7 -delete 2>/dev/null || true
        
    - name: Send notification on failure
      if: failure()
      run: |
        echo "❌ Crawling failed. Check logs for details."
        # You can add webhook notification here if needed