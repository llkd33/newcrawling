#!/usr/bin/env python3
"""
í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°
Task 11ì˜ ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ì¢…í•© ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

Requirements covered:
- 1.1, 1.2, 1.3: ë‹¤ì–‘í•œ ì—ë””í„° í˜•ì‹ í…ŒìŠ¤íŠ¸
- 2.1, 2.2, 2.3: ë™ì  ì½˜í…ì¸  ë¡œë”© ë° ë„¤íŠ¸ì›Œí¬ ì§€ì—° í…ŒìŠ¤íŠ¸
"""

import os
import sys
import time
import json
import logging
import subprocess
from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path

# í…ŒìŠ¤íŠ¸ ëª¨ë“ˆë“¤ import
from test_integration_real_data import IntegrationTestSuite
from test_performance_load import PerformanceTestSuite

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


class IntegrationTestRunner:
    """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.results = {
            'start_time': datetime.now().isoformat(),
            'test_suites': [],
            'summary': {
                'total_suites': 0,
                'passed_suites': 0,
                'failed_suites': 0,
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 0
            },
            'environment': {
                'python_version': sys.version,
                'github_actions': os.getenv('GITHUB_ACTIONS', '').lower() == 'true',
                'headless_mode': True,
                'test_timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')
            }
        }
    
    def check_prerequisites(self) -> bool:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì „ í•„ìˆ˜ ì¡°ê±´ í™•ì¸"""
        self.logger.info("ğŸ” í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì „ í•„ìˆ˜ ì¡°ê±´ í™•ì¸")
        
        # í™˜ê²½ë³€ìˆ˜ í™•ì¸
        required_env = ['NAVER_ID', 'NAVER_PW']
        missing_env = [env for env in required_env if not os.getenv(env)]
        
        if missing_env:
            self.logger.error(f"âŒ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {', '.join(missing_env)}")
            self.logger.info("ğŸ’¡ .env íŒŒì¼ì— ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:")
            for env in missing_env:
                self.logger.info(f"   {env}=your_value")
            return False
        
        # í•„ìˆ˜ ëª¨ë“ˆ í™•ì¸
        required_modules = [
            'selenium', 'pytest', 'psutil', 'dotenv'
        ]
        
        missing_modules = []
        for module in required_modules:
            try:
                __import__(module)
            except ImportError:
                missing_modules.append(module)
        
        if missing_modules:
            self.logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ ëˆ„ë½: {', '.join(missing_modules)}")
            self.logger.info("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
            self.logger.info(f"   pip install {' '.join(missing_modules)}")
            return False
        
        # Chrome ë“œë¼ì´ë²„ í™•ì¸
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            
            options = Options()
            options.add_argument('--headless=new')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            
            driver = webdriver.Chrome(options=options)
            driver.quit()
            
            self.logger.info("âœ… Chrome ë“œë¼ì´ë²„ í™•ì¸ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ Chrome ë“œë¼ì´ë²„ ì˜¤ë¥˜: {e}")
            self.logger.info("ğŸ’¡ Chromeê³¼ ChromeDriverê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            return False
        
        self.logger.info("âœ… ëª¨ë“  í•„ìˆ˜ ì¡°ê±´ í™•ì¸ ì™„ë£Œ")
        return True
    
    def run_real_data_tests(self) -> Dict[str, Any]:
        """ì‹¤ì œ ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸ§ª ì‹¤ì œ ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        try:
            test_suite = IntegrationTestSuite()
            results = test_suite.run_all_tests(headless=True)
            
            # ê²°ê³¼ ì •ê·œí™”
            suite_result = {
                'suite_name': 'real_data_integration',
                'success': results['summary']['passed_tests'] > 0,
                'total_tests': results['summary']['total_tests'],
                'passed_tests': results['summary']['passed_tests'],
                'failed_tests': results['summary']['failed_tests'],
                'details': results,
                'duration': self._calculate_duration(results.get('start_time'), results.get('end_time'))
            }
            
            return suite_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                'suite_name': 'real_data_integration',
                'success': False,
                'error': str(e),
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 1
            }
    
    def run_performance_tests(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        try:
            test_suite = PerformanceTestSuite()
            results = test_suite.run_all_performance_tests()
            
            # ê²°ê³¼ ì •ê·œí™”
            suite_result = {
                'suite_name': 'performance_load',
                'success': results['summary']['passed_tests'] >= results['summary']['total_tests'] * 0.7,
                'total_tests': results['summary']['total_tests'],
                'passed_tests': results['summary']['passed_tests'],
                'failed_tests': results['summary']['failed_tests'],
                'details': results,
                'duration': results.get('total_duration', 0)
            }
            
            return suite_result
            
        except Exception as e:
            self.logger.error(f"âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                'suite_name': 'performance_load',
                'success': False,
                'error': str(e),
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 1
            }
    
    def run_pytest_tests(self) -> Dict[str, Any]:
        """pytest ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸ§ª pytest ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        try:
            # pytest ì‹¤í–‰
            cmd = [
                sys.executable, '-m', 'pytest', 
                'test_editor_formats.py',
                '-v', '--tb=short', '--json-report', '--json-report-file=pytest_report.json'
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            
            # pytest ê²°ê³¼ íŒŒì‹±
            try:
                with open('pytest_report.json', 'r') as f:
                    pytest_data = json.load(f)
                
                suite_result = {
                    'suite_name': 'pytest_editor_formats',
                    'success': result.returncode == 0,
                    'total_tests': pytest_data['summary']['total'],
                    'passed_tests': pytest_data['summary']['passed'],
                    'failed_tests': pytest_data['summary']['failed'],
                    'details': pytest_data,
                    'duration': pytest_data['duration']
                }
                
            except (FileNotFoundError, json.JSONDecodeError, KeyError):
                # pytest ë³´ê³ ì„œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ ì‚¬ìš©
                suite_result = {
                    'suite_name': 'pytest_editor_formats',
                    'success': result.returncode == 0,
                    'total_tests': 1,
                    'passed_tests': 1 if result.returncode == 0 else 0,
                    'failed_tests': 0 if result.returncode == 0 else 1,
                    'stdout': result.stdout,
                    'stderr': result.stderr
                }
            
            return suite_result
            
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ pytest í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ")
            return {
                'suite_name': 'pytest_editor_formats',
                'success': False,
                'error': 'Timeout',
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 1
            }
        except Exception as e:
            self.logger.error(f"âŒ pytest í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                'suite_name': 'pytest_editor_formats',
                'success': False,
                'error': str(e),
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 1
            }
    
    def _calculate_duration(self, start_time: str, end_time: str) -> float:
        """ì‹œê°„ ì°¨ì´ ê³„ì‚°"""
        try:
            if start_time and end_time:
                start = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                end = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
                return (end - start).total_seconds()
        except:
            pass
        return 0
    
    def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("="*80)
        self.logger.info("ğŸš€ ë„¤ì´ë²„ ì¹´í˜ ì½˜í…ì¸  ì¶”ì¶œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        self.logger.info(f"â° {datetime.now()}")
        self.logger.info("="*80)
        
        # í•„ìˆ˜ ì¡°ê±´ í™•ì¸
        if not self.check_prerequisites():
            self.results['fatal_error'] = "í•„ìˆ˜ ì¡°ê±´ ë¯¸ì¶©ì¡±"
            return self.results
        
        # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ë“¤ ì‹¤í–‰
        test_suites = [
            ('ì‹¤ì œ ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸', self.run_real_data_tests),
            ('ì„±ëŠ¥ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸', self.run_performance_tests),
            ('pytest ì—ë””í„° í˜•ì‹ í…ŒìŠ¤íŠ¸', self.run_pytest_tests)
        ]
        
        for suite_name, suite_method in test_suites:
            self.logger.info(f"\nğŸ“‹ ì‹¤í–‰ ì¤‘: {suite_name}")
            
            try:
                suite_result = suite_method()
                self.results['test_suites'].append(suite_result)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.results['summary']['total_suites'] += 1
                self.results['summary']['total_tests'] += suite_result.get('total_tests', 0)
                self.results['summary']['passed_tests'] += suite_result.get('passed_tests', 0)
                self.results['summary']['failed_tests'] += suite_result.get('failed_tests', 0)
                
                if suite_result.get('success', False):
                    self.results['summary']['passed_suites'] += 1
                    self.logger.info(f"âœ… {suite_name}: ì„±ê³µ")
                else:
                    self.results['summary']['failed_suites'] += 1
                    self.logger.warning(f"âŒ {suite_name}: ì‹¤íŒ¨")
                
            except Exception as e:
                self.logger.error(f"âŒ {suite_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                self.results['test_suites'].append({
                    'suite_name': suite_name,
                    'success': False,
                    'error': str(e),
                    'total_tests': 0,
                    'passed_tests': 0,
                    'failed_tests': 1
                })
                self.results['summary']['failed_suites'] += 1
                self.results['summary']['total_tests'] += 1
                self.results['summary']['failed_tests'] += 1
        
        self.results['end_time'] = datetime.now().isoformat()
        return self.results
    
    def generate_comprehensive_report(self) -> str:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("="*100)
        report.append("ğŸ“Š ë„¤ì´ë²„ ì¹´í˜ ì½˜í…ì¸  ì¶”ì¶œ í†µí•© í…ŒìŠ¤íŠ¸ ì¢…í•© ë³´ê³ ì„œ")
        report.append("="*100)
        
        # ì „ì²´ ìš”ì•½
        summary = self.results['summary']
        report.append(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
        report.append(f"  â€¢ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸: {summary['total_suites']}ê°œ (ì„±ê³µ: {summary['passed_suites']}, ì‹¤íŒ¨: {summary['failed_suites']})")
        report.append(f"  â€¢ ê°œë³„ í…ŒìŠ¤íŠ¸: {summary['total_tests']}ê°œ (ì„±ê³µ: {summary['passed_tests']}, ì‹¤íŒ¨: {summary['failed_tests']})")
        
        if summary['total_tests'] > 0:
            success_rate = (summary['passed_tests'] / summary['total_tests']) * 100
            report.append(f"  â€¢ ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        # í™˜ê²½ ì •ë³´
        env = self.results['environment']
        report.append(f"\nğŸ”§ í…ŒìŠ¤íŠ¸ í™˜ê²½:")
        report.append(f"  â€¢ Python ë²„ì „: {env['python_version'].split()[0]}")
        report.append(f"  â€¢ GitHub Actions: {env['github_actions']}")
        report.append(f"  â€¢ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ: {env['headless_mode']}")
        report.append(f"  â€¢ í…ŒìŠ¤íŠ¸ ì‹œê°„: {env['test_timestamp']}")
        
        # ê° í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ê²°ê³¼
        report.append(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ë³„ ìƒì„¸ ê²°ê³¼:")
        
        for suite in self.results['test_suites']:
            suite_name = suite.get('suite_name', 'Unknown')
            success_icon = "âœ…" if suite.get('success', False) else "âŒ"
            
            report.append(f"\n{success_icon} {suite_name}:")
            report.append(f"  â€¢ ì´ í…ŒìŠ¤íŠ¸: {suite.get('total_tests', 0)}ê°œ")
            report.append(f"  â€¢ ì„±ê³µ: {suite.get('passed_tests', 0)}ê°œ")
            report.append(f"  â€¢ ì‹¤íŒ¨: {suite.get('failed_tests', 0)}ê°œ")
            
            if 'duration' in suite:
                report.append(f"  â€¢ ì†Œìš” ì‹œê°„: {suite['duration']:.1f}ì´ˆ")
            
            if 'error' in suite:
                report.append(f"  â€¢ ì˜¤ë¥˜: {suite['error']}")
        
        # Requirements ì¶©ì¡± ì—¬ë¶€ í™•ì¸
        report.append(f"\nâœ… Requirements ì¶©ì¡± ì—¬ë¶€:")
        
        # Requirement 1.1, 1.2, 1.3 (ì—ë””í„° í˜•ì‹)
        editor_tests = [s for s in self.results['test_suites'] 
                       if 'editor' in s.get('suite_name', '').lower()]
        editor_success = any(s.get('success', False) for s in editor_tests)
        
        report.append(f"  â€¢ Req 1.1-1.3 (ë‹¤ì–‘í•œ ì—ë””í„° í˜•ì‹): {'âœ… ì¶©ì¡±' if editor_success else 'âŒ ë¯¸ì¶©ì¡±'}")
        
        # Requirement 2.1, 2.2, 2.3 (ë™ì  ì½˜í…ì¸  ë¡œë”©)
        loading_tests = [s for s in self.results['test_suites'] 
                        if any(keyword in s.get('suite_name', '').lower() 
                              for keyword in ['real_data', 'performance'])]
        loading_success = any(s.get('success', False) for s in loading_tests)
        
        report.append(f"  â€¢ Req 2.1-2.3 (ë™ì  ì½˜í…ì¸  ë¡œë”©): {'âœ… ì¶©ì¡±' if loading_success else 'âŒ ë¯¸ì¶©ì¡±'}")
        
        # ê¶Œì¥ì‚¬í•­
        report.append(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        
        if summary['failed_suites'] > 0:
            report.append("  â€¢ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œìŠ¤í…œì„ ê°œì„ í•˜ì„¸ìš”")
        
        if summary['total_tests'] > 0:
            success_rate = (summary['passed_tests'] / summary['total_tests']) * 100
            if success_rate < 70:
                report.append("  â€¢ ì „ì²´ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ì ê²€í•˜ì„¸ìš”")
            elif success_rate >= 90:
                report.append("  â€¢ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ìš°ìˆ˜í•©ë‹ˆë‹¤. í˜„ì¬ êµ¬í˜„ì„ ìœ ì§€í•˜ì„¸ìš”")
            else:
                report.append("  â€¢ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤. ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        # Task 11 ì™„ë£Œ ìƒíƒœ
        task_complete = (
            summary['passed_suites'] >= 2 and  # ìµœì†Œ 2ê°œ ìŠ¤ìœ„íŠ¸ ì„±ê³µ
            editor_success and                  # ì—ë””í„° í˜•ì‹ í…ŒìŠ¤íŠ¸ ì„±ê³µ
            loading_success                     # ë™ì  ë¡œë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ
        )
        
        report.append(f"\nğŸ¯ Task 11 ì™„ë£Œ ìƒíƒœ: {'âœ… ì™„ë£Œ' if task_complete else 'âŒ ë¯¸ì™„ë£Œ'}")
        
        if task_complete:
            report.append("  â€¢ ì‹¤ì œ ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œë¬¼ ëŒ€ìƒ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            report.append("  â€¢ ë‹¤ì–‘í•œ ì—ë””í„° í˜•ì‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            report.append("  â€¢ ë„¤íŠ¸ì›Œí¬ ì§€ì—° ë° ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        report.append("="*100)
        
        return "\n".join(report)
    
    def save_results(self) -> str:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = self.results['environment']['test_timestamp']
        
        # JSON ê²°ê³¼ ì €ì¥
        json_file = f"integration_test_results_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥
        report = self.generate_comprehensive_report()
        txt_file = f"integration_test_report_{timestamp}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return txt_file


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    runner = IntegrationTestRunner()
    
    try:
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = runner.run_all_tests()
        
        # ë³´ê³ ì„œ ìƒì„± ë° ì¶œë ¥
        report = runner.generate_comprehensive_report()
        print(report)
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        report_file = runner.save_results()
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_file}")
        
        # GitHub Actionsë¥¼ ìœ„í•œ ì•„í‹°íŒ©íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        if os.getenv('GITHUB_ACTIONS'):
            os.makedirs('artifacts', exist_ok=True)
            
            # ì¤‘ìš” íŒŒì¼ë“¤ì„ artifactsë¡œ ë³µì‚¬
            import shutil
            for file in [report_file, f"integration_test_results_{results['environment']['test_timestamp']}.json"]:
                if os.path.exists(file):
                    shutil.copy2(file, 'artifacts/')
        
        # ì„±ê³µ ì—¬ë¶€ íŒì •
        summary = results['summary']
        if summary['total_tests'] > 0:
            success_rate = (summary['passed_tests'] / summary['total_tests']) * 100
            task_success = (
                summary['passed_suites'] >= 2 and
                success_rate >= 70
            )
        else:
            task_success = False
        
        if task_success:
            print("\nğŸ‰ Task 11 í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            return True
        else:
            print("\nâŒ Task 11 í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return False
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        return False
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)