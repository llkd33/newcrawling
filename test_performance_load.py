#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸
ë„¤íŠ¸ì›Œí¬ ì§€ì—°, ë™ì‹œ ìš”ì²­, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“±ì„ í…ŒìŠ¤íŠ¸
"""

import time
import threading
import psutil
import gc
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
from dataclasses import dataclass
from statistics import mean, median, stdev

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait

from content_extractor import ContentExtractor
from content_extraction_models import ExtractionConfig

logging.basicConfig(level=logging.INFO)


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    extraction_time: float
    memory_usage_mb: float
    success: bool
    content_length: int
    extraction_method: str
    error_message: str = None


class PerformanceTestSuite:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.test_urls = [
            "https://cafe.naver.com/f-e/cafes/18786605/articles/1941841?boardtype=L&menuid=105",
            # ì¶”ê°€ í…ŒìŠ¤íŠ¸ URLë“¤ì„ ì—¬ê¸°ì— ì¶”ê°€
        ]
    
    def create_driver(self, headless: bool = True) -> tuple:
        """í…ŒìŠ¤íŠ¸ìš© ë“œë¼ì´ë²„ ìƒì„±"""
        options = Options()
        
        if headless:
            options.add_argument('--headless=new')
        
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        options.add_argument('--memory-pressure-off')
        options.add_argument('--max_old_space_size=4096')
        
        driver = webdriver.Chrome(options=options)
        wait = WebDriverWait(driver, 30)
        
        return driver, wait
    
    def measure_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def single_extraction_test(self, url: str, config: ExtractionConfig) -> PerformanceMetrics:
        """ë‹¨ì¼ ì¶”ì¶œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        driver, wait = self.create_driver()
        
        try:
            # ë„¤ì´ë²„ ë¡œê·¸ì¸ (ê°„ì†Œí™”)
            driver.get('https://nid.naver.com/nidlogin.login')
            time.sleep(2)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
            memory_before = self.measure_memory_usage()
            
            # ContentExtractor ìƒì„±
            extractor = ContentExtractor(driver, wait, config)
            
            # ì¶”ì¶œ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            result = extractor.extract_content(url)
            extraction_time = time.time() - start_time
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì¢…ë£Œ
            memory_after = self.measure_memory_usage()
            memory_usage = memory_after - memory_before
            
            return PerformanceMetrics(
                extraction_time=extraction_time,
                memory_usage_mb=memory_usage,
                success=result.success,
                content_length=len(result.content),
                extraction_method=result.extraction_method.value,
                error_message=result.error_message
            )
            
        except Exception as e:
            return PerformanceMetrics(
                extraction_time=0,
                memory_usage_mb=0,
                success=False,
                content_length=0,
                extraction_method="error",
                error_message=str(e)
            )
        
        finally:
            driver.quit()
            gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    
    def test_extraction_speed(self, iterations: int = 10) -> Dict[str, Any]:
        """ì¶”ì¶œ ì†ë„ í…ŒìŠ¤íŠ¸ (Requirements: 2.1, 2.2, 2.3)"""
        self.logger.info(f"ğŸš€ ì¶”ì¶œ ì†ë„ í…ŒìŠ¤íŠ¸ ì‹œì‘ ({iterations}íšŒ ë°˜ë³µ)")
        
        config = ExtractionConfig(timeout_seconds=30)
        metrics = []
        
        for i in range(iterations):
            self.logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ {i+1}/{iterations}")
            
            for url in self.test_urls:
                metric = self.single_extraction_test(url, config)
                metrics.append(metric)
                
                self.logger.info(f"  â±ï¸ ì‹œê°„: {metric.extraction_time:.2f}ì´ˆ, "
                               f"ë©”ëª¨ë¦¬: {metric.memory_usage_mb:.1f}MB, "
                               f"ì„±ê³µ: {metric.success}")
                
                time.sleep(1)  # í…ŒìŠ¤íŠ¸ ê°„ ê°„ê²©
        
        # í†µê³„ ê³„ì‚°
        successful_metrics = [m for m in metrics if m.success]
        
        if not successful_metrics:
            return {
                'test_name': 'extraction_speed',
                'status': 'FAILED',
                'error': 'ëª¨ë“  ì¶”ì¶œ ì‹¤íŒ¨'
            }
        
        extraction_times = [m.extraction_time for m in successful_metrics]
        memory_usages = [m.memory_usage_mb for m in successful_metrics]
        
        return {
            'test_name': 'extraction_speed',
            'iterations': iterations,
            'successful_extractions': len(successful_metrics),
            'failed_extractions': len(metrics) - len(successful_metrics),
            'success_rate': len(successful_metrics) / len(metrics) * 100,
            'extraction_time_stats': {
                'mean': mean(extraction_times),
                'median': median(extraction_times),
                'min': min(extraction_times),
                'max': max(extraction_times),
                'std_dev': stdev(extraction_times) if len(extraction_times) > 1 else 0
            },
            'memory_usage_stats': {
                'mean': mean(memory_usages),
                'median': median(memory_usages),
                'min': min(memory_usages),
                'max': max(memory_usages),
                'std_dev': stdev(memory_usages) if len(memory_usages) > 1 else 0
            }
        }
    
    def test_timeout_scenarios(self) -> Dict[str, Any]:
        """ë‹¤ì–‘í•œ íƒ€ì„ì•„ì›ƒ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        self.logger.info("â° íƒ€ì„ì•„ì›ƒ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        timeout_scenarios = [
            {'name': 'ë§¤ìš° ì§§ì€ íƒ€ì„ì•„ì›ƒ', 'timeout': 5, 'expected_failures': True},
            {'name': 'ì§§ì€ íƒ€ì„ì•„ì›ƒ', 'timeout': 15, 'expected_failures': False},
            {'name': 'ì¼ë°˜ íƒ€ì„ì•„ì›ƒ', 'timeout': 30, 'expected_failures': False},
            {'name': 'ê¸´ íƒ€ì„ì•„ì›ƒ', 'timeout': 60, 'expected_failures': False}
        ]
        
        results = []
        
        for scenario in timeout_scenarios:
            self.logger.info(f"â±ï¸ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']} ({scenario['timeout']}ì´ˆ)")
            
            config = ExtractionConfig(timeout_seconds=scenario['timeout'])
            
            # ê° ì‹œë‚˜ë¦¬ì˜¤ë‹¹ 3íšŒ í…ŒìŠ¤íŠ¸
            scenario_metrics = []
            for i in range(3):
                metric = self.single_extraction_test(self.test_urls[0], config)
                scenario_metrics.append(metric)
            
            successful = [m for m in scenario_metrics if m.success]
            success_rate = len(successful) / len(scenario_metrics) * 100
            
            avg_time = mean([m.extraction_time for m in successful]) if successful else 0
            
            scenario_result = {
                'name': scenario['name'],
                'timeout_seconds': scenario['timeout'],
                'success_rate': success_rate,
                'average_time': avg_time,
                'expected_failures': scenario['expected_failures'],
                'meets_expectation': (
                    (scenario['expected_failures'] and success_rate < 50) or
                    (not scenario['expected_failures'] and success_rate >= 70)
                )
            }
            
            results.append(scenario_result)
            
            self.logger.info(f"  ğŸ“Š ì„±ê³µë¥ : {success_rate:.1f}%, í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ")
        
        return {
            'test_name': 'timeout_scenarios',
            'scenarios': results,
            'overall_success': all(r['meets_expectation'] for r in results)
        }
    
    def test_concurrent_extractions(self, max_workers: int = 3) -> Dict[str, Any]:
        """ë™ì‹œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        self.logger.info(f"ğŸ”„ ë™ì‹œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì›Œì»¤: {max_workers}ê°œ)")
        
        def worker_task(worker_id: int) -> Dict[str, Any]:
            """ì›Œì»¤ íƒœìŠ¤í¬"""
            self.logger.info(f"ğŸ‘· ì›Œì»¤ {worker_id} ì‹œì‘")
            
            config = ExtractionConfig(timeout_seconds=45)  # ë™ì‹œ ì‹¤í–‰ ì‹œ ì—¬ìœ ìˆê²Œ
            
            start_time = time.time()
            metric = self.single_extraction_test(self.test_urls[0], config)
            total_time = time.time() - start_time
            
            return {
                'worker_id': worker_id,
                'total_time': total_time,
                'extraction_time': metric.extraction_time,
                'success': metric.success,
                'memory_usage': metric.memory_usage_mb,
                'error': metric.error_message
            }
        
        # ë™ì‹œ ì‹¤í–‰
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(worker_task, i) for i in range(max_workers)]
            worker_results = []
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    worker_results.append(result)
                    self.logger.info(f"âœ… ì›Œì»¤ {result['worker_id']} ì™„ë£Œ: {result['success']}")
                except Exception as e:
                    self.logger.error(f"âŒ ì›Œì»¤ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                    worker_results.append({
                        'worker_id': -1,
                        'success': False,
                        'error': str(e)
                    })
        
        total_concurrent_time = time.time() - start_time
        
        successful_workers = [r for r in worker_results if r['success']]
        success_rate = len(successful_workers) / len(worker_results) * 100
        
        return {
            'test_name': 'concurrent_extractions',
            'max_workers': max_workers,
            'total_concurrent_time': total_concurrent_time,
            'success_rate': success_rate,
            'successful_workers': len(successful_workers),
            'failed_workers': len(worker_results) - len(successful_workers),
            'worker_results': worker_results,
            'average_extraction_time': mean([r['extraction_time'] for r in successful_workers]) if successful_workers else 0
        }
    
    def test_memory_leak_detection(self, iterations: int = 20) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        self.logger.info(f"ğŸ§  ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ í…ŒìŠ¤íŠ¸ ì‹œì‘ ({iterations}íšŒ)")
        
        config = ExtractionConfig(timeout_seconds=20)
        memory_snapshots = []
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
        initial_memory = self.measure_memory_usage()
        memory_snapshots.append(initial_memory)
        
        for i in range(iterations):
            self.logger.info(f"ğŸ” ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ {i+1}/{iterations}")
            
            # ì¶”ì¶œ ì‹¤í–‰
            metric = self.single_extraction_test(self.test_urls[0], config)
            
            # ë©”ëª¨ë¦¬ ì¸¡ì •
            current_memory = self.measure_memory_usage()
            memory_snapshots.append(current_memory)
            
            self.logger.info(f"  ğŸ“Š ë©”ëª¨ë¦¬: {current_memory:.1f}MB (+{current_memory - initial_memory:.1f}MB)")
            
            # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            time.sleep(1)
        
        # ë©”ëª¨ë¦¬ ì¦ê°€ ë¶„ì„
        memory_increases = [memory_snapshots[i] - memory_snapshots[i-1] 
                           for i in range(1, len(memory_snapshots))]
        
        total_increase = memory_snapshots[-1] - memory_snapshots[0]
        average_increase = mean(memory_increases)
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íŒì • (ì„ê³„ê°’: 100MB ì´ìƒ ì¦ê°€)
        has_memory_leak = total_increase > 100
        
        return {
            'test_name': 'memory_leak_detection',
            'iterations': iterations,
            'initial_memory_mb': initial_memory,
            'final_memory_mb': memory_snapshots[-1],
            'total_increase_mb': total_increase,
            'average_increase_mb': average_increase,
            'max_increase_mb': max(memory_increases),
            'has_memory_leak': has_memory_leak,
            'memory_snapshots': memory_snapshots
        }
    
    def test_network_conditions(self) -> Dict[str, Any]:
        """ë‹¤ì–‘í•œ ë„¤íŠ¸ì›Œí¬ ì¡°ê±´ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ğŸŒ ë„¤íŠ¸ì›Œí¬ ì¡°ê±´ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ë„¤íŠ¸ì›Œí¬ ì¡°ê±´ë³„ í…ŒìŠ¤íŠ¸ëŠ” ì‹¤ì œ í™˜ê²½ì—ì„œë§Œ ì˜ë¯¸ê°€ ìˆìœ¼ë¯€ë¡œ
        # ì—¬ê¸°ì„œëŠ” íƒ€ì„ì•„ì›ƒ ì¡°ì •ì„ í†µí•œ ê°„ì ‘ í…ŒìŠ¤íŠ¸
        
        network_scenarios = [
            {'name': 'ë¹ ë¥¸ ë„¤íŠ¸ì›Œí¬', 'timeout': 15, 'expected_success_rate': 90},
            {'name': 'ë³´í†µ ë„¤íŠ¸ì›Œí¬', 'timeout': 30, 'expected_success_rate': 95},
            {'name': 'ëŠë¦° ë„¤íŠ¸ì›Œí¬', 'timeout': 60, 'expected_success_rate': 80}
        ]
        
        results = []
        
        for scenario in network_scenarios:
            self.logger.info(f"ğŸ“¡ ë„¤íŠ¸ì›Œí¬ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
            
            config = ExtractionConfig(timeout_seconds=scenario['timeout'])
            
            # ì‹œë‚˜ë¦¬ì˜¤ë‹¹ 5íšŒ í…ŒìŠ¤íŠ¸
            scenario_results = []
            for i in range(5):
                metric = self.single_extraction_test(self.test_urls[0], config)
                scenario_results.append(metric)
            
            successful = [r for r in scenario_results if r.success]
            success_rate = len(successful) / len(scenario_results) * 100
            avg_time = mean([r.extraction_time for r in successful]) if successful else 0
            
            scenario_result = {
                'name': scenario['name'],
                'timeout': scenario['timeout'],
                'success_rate': success_rate,
                'average_time': avg_time,
                'expected_success_rate': scenario['expected_success_rate'],
                'meets_expectation': success_rate >= scenario['expected_success_rate'] * 0.8  # 80% í—ˆìš©
            }
            
            results.append(scenario_result)
            
            self.logger.info(f"  ğŸ“Š ì„±ê³µë¥ : {success_rate:.1f}% (ê¸°ëŒ€: {scenario['expected_success_rate']}%)")
        
        return {
            'test_name': 'network_conditions',
            'scenarios': results,
            'overall_success': all(r['meets_expectation'] for r in results)
        }
    
    def run_all_performance_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("="*60)
        self.logger.info("ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹œì‘")
        self.logger.info("="*60)
        
        results = {
            'start_time': time.time(),
            'tests': [],
            'summary': {
                'total_tests': 0,
                'passed_tests': 0,
                'failed_tests': 0
            }
        }
        
        # ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ë“¤
        test_methods = [
            ('extraction_speed', lambda: self.test_extraction_speed(5)),
            ('timeout_scenarios', self.test_timeout_scenarios),
            ('concurrent_extractions', lambda: self.test_concurrent_extractions(2)),
            ('memory_leak_detection', lambda: self.test_memory_leak_detection(10)),
            ('network_conditions', self.test_network_conditions)
        ]
        
        for test_name, test_method in test_methods:
            try:
                self.logger.info(f"\nğŸ§ª ì‹¤í–‰ ì¤‘: {test_name}")
                test_result = test_method()
                
                # ì„±ê³µ/ì‹¤íŒ¨ íŒì •
                if 'overall_success' in test_result:
                    success = test_result['overall_success']
                elif 'success_rate' in test_result:
                    success = test_result['success_rate'] >= 70
                elif 'has_memory_leak' in test_result:
                    success = not test_result['has_memory_leak']
                else:
                    success = test_result.get('status') != 'FAILED'
                
                test_result['success'] = success
                results['tests'].append(test_result)
                
                if success:
                    results['summary']['passed_tests'] += 1
                    self.logger.info(f"âœ… {test_name}: ì„±ê³µ")
                else:
                    results['summary']['failed_tests'] += 1
                    self.logger.warning(f"âŒ {test_name}: ì‹¤íŒ¨")
                
                results['summary']['total_tests'] += 1
                
            except Exception as e:
                self.logger.error(f"âŒ {test_name}: ì˜ˆì™¸ ë°œìƒ - {e}")
                results['tests'].append({
                    'test_name': test_name,
                    'success': False,
                    'error': str(e)
                })
                results['summary']['failed_tests'] += 1
                results['summary']['total_tests'] += 1
        
        results['end_time'] = time.time()
        results['total_duration'] = results['end_time'] - results['start_time']
        
        return results
    
    def generate_performance_report(self, results: Dict[str, Any]) -> str:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("="*80)
        report.append("ğŸ“Š ë„¤ì´ë²„ ì¹´í˜ ì½˜í…ì¸  ì¶”ì¶œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ")
        report.append("="*80)
        
        # ìš”ì•½
        summary = results['summary']
        report.append(f"\nğŸ“ˆ í…ŒìŠ¤íŠ¸ ìš”ì•½:")
        report.append(f"  â€¢ ì´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}ê°œ")
        report.append(f"  â€¢ ì„±ê³µ: {summary['passed_tests']}ê°œ")
        report.append(f"  â€¢ ì‹¤íŒ¨: {summary['failed_tests']}ê°œ")
        report.append(f"  â€¢ ì „ì²´ ì†Œìš” ì‹œê°„: {results['total_duration']:.1f}ì´ˆ")
        
        # ê° í…ŒìŠ¤íŠ¸ ìƒì„¸ ê²°ê³¼
        for test in results['tests']:
            test_name = test.get('test_name', 'Unknown')
            report.append(f"\nğŸ“‹ {test_name}:")
            
            if test_name == 'extraction_speed':
                stats = test.get('extraction_time_stats', {})
                report.append(f"  â€¢ í‰ê·  ì¶”ì¶œ ì‹œê°„: {stats.get('mean', 0):.2f}ì´ˆ")
                report.append(f"  â€¢ ìµœì†Œ/ìµœëŒ€ ì‹œê°„: {stats.get('min', 0):.2f}ì´ˆ / {stats.get('max', 0):.2f}ì´ˆ")
                report.append(f"  â€¢ ì„±ê³µë¥ : {test.get('success_rate', 0):.1f}%")
            
            elif test_name == 'concurrent_extractions':
                report.append(f"  â€¢ ë™ì‹œ ì›Œì»¤: {test.get('max_workers', 0)}ê°œ")
                report.append(f"  â€¢ ì „ì²´ ì†Œìš” ì‹œê°„: {test.get('total_concurrent_time', 0):.2f}ì´ˆ")
                report.append(f"  â€¢ ì„±ê³µë¥ : {test.get('success_rate', 0):.1f}%")
            
            elif test_name == 'memory_leak_detection':
                report.append(f"  â€¢ ë©”ëª¨ë¦¬ ì¦ê°€: {test.get('total_increase_mb', 0):.1f}MB")
                report.append(f"  â€¢ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜: {'ìˆìŒ' if test.get('has_memory_leak', False) else 'ì—†ìŒ'}")
            
            status = "âœ… ì„±ê³µ" if test.get('success', False) else "âŒ ì‹¤íŒ¨"
            report.append(f"  â€¢ ê²°ê³¼: {status}")
        
        report.append("="*80)
        
        return "\n".join(report)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    test_suite = PerformanceTestSuite()
    results = test_suite.run_all_performance_tests()
    
    # ë³´ê³ ì„œ ìƒì„± ë° ì¶œë ¥
    report = test_suite.generate_performance_report(results)
    print(report)
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    report_file = f"performance_test_report_{timestamp}.txt"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_file}")
    
    # ì„±ê³µë¥  ê¸°ì¤€ìœ¼ë¡œ ì¢…ë£Œ ì½”ë“œ ê²°ì •
    success_rate = (results['summary']['passed_tests'] / 
                   max(results['summary']['total_tests'], 1)) * 100
    
    return success_rate >= 70


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)